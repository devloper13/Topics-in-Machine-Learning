{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "cartpole_env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a model\n",
    "def create_model(input_dim,output_dim):\n",
    "   \n",
    "    \n",
    "    input1 = keras.layers.Input(input_dim)\n",
    "    hidden1 = keras.layers.Dense(10,activation='relu',name=\"layer1\")(input1)\n",
    "    hidden2 = keras.layers.Dense(4,activation='relu',name=\"layer2\")(hidden1)\n",
    "    if output_dim == 1:\n",
    "        output1 = keras.layers.Dense(output_dim,activation='tanh',name=\"output\")(hidden2)\n",
    "    else:\n",
    "        output1 = keras.layers.Dense(output_dim,activation='softmax',name=\"output\")(hidden2)\n",
    "    \n",
    "   \n",
    "    model = keras.models.Model(inputs=input1, outputs=output1)  \n",
    "    return model\n",
    "   \n",
    "\n",
    "def build_train_fn(model):\n",
    "    action_prob_placeholder = model.output\n",
    "    action_onehot_placeholder = keras.backend.placeholder(shape=(None, output_dim),name=\"action_onehot\")\n",
    "    discount_reward_placeholder = keras.backend.placeholder(shape=(None,),name=\"discount_reward\")\n",
    "    \n",
    "    if output_dim > 1:\n",
    "        action_prob = keras.backend.sum(action_prob_placeholder * action_onehot_placeholder, axis=1)\n",
    "    else:\n",
    "        action_prob = action_prob_placeholder\n",
    "        \n",
    "    log_action_prob = keras.backend.log(action_prob)\n",
    "\n",
    "    loss = -log_action_prob * discount_reward_placeholder\n",
    "    loss = keras.backend.mean(loss)\n",
    "\n",
    "    adam = keras.optimizers.Adam()\n",
    "\n",
    "    updates = adam.get_updates(params=model.trainable_weights,loss=loss)\n",
    "    \n",
    "    if output_dim > 1:\n",
    "        train_fn = keras.backend.function(inputs=[model.input,action_onehot_placeholder,discount_reward_placeholder],\n",
    "                                           outputs=[],\n",
    "                                           updates=updates) \n",
    "    else:\n",
    "        train_fn = keras.backend.function(inputs=[model.input,discount_reward_placeholder],\n",
    "                                           outputs=[],\n",
    "                                           updates=updates)\n",
    "    return train_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(states,actions,rewards,model_train):\n",
    "    discount_reward = discounted_reward(rewards)\n",
    "    if output_dim > 1:\n",
    "        action_onehot = keras.utils.to_categorical(actions, num_classes=output_dim)\n",
    "        model_train([states, action_onehot, discount_reward])\n",
    "    else:\n",
    "        model_train([states, discount_reward])\n",
    "    \n",
    "\n",
    "def discounted_reward(rewards):\n",
    "    size = len(rewards)\n",
    "    cum_reward = [0]*size\n",
    "    cum=0\n",
    "    for t in range(size-1,-1,-1):\n",
    "        cum+=rewards[t]\n",
    "        cum_reward[t] = cum\n",
    "    \n",
    "    disc = 1\n",
    "    for t in range(0,size):\n",
    "        cum_reward[t] = disc * cum_reward[t]\n",
    "        disc *= gamma\n",
    "    cum_reward = np.array(cum_reward)\n",
    "    cum_reward = (cum_reward - cum_reward.mean()) / cum_reward.std()\n",
    "    return cum_reward\n",
    "    \n",
    "def get_action(state,model,test=0):\n",
    "    if len(state.shape) == 1:\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "    \n",
    "    #print(np.arange(total_actions))\n",
    "    #print(action_dist)\n",
    "    if(output_dim > 1):\n",
    "        action_dist = np.squeeze(model.predict(state))\n",
    "        if test == 1:\n",
    "            return np.argmax(action_dist)\n",
    "        return np.random.choice(np.arange(total_actions),p=action_dist)\n",
    "    else:\n",
    "        \n",
    "        action_dist = model.predict(state)\n",
    "        #for single dimension action regardless of test or not\n",
    "        return action_dist\n",
    "\n",
    "def reinforce(env,model,model_train):\n",
    "    \n",
    "    for episode in range(1,episodes+1):\n",
    "        done = False\n",
    "        rewards = []\n",
    "        actions = []\n",
    "        states = []\n",
    "        state = env.reset()\n",
    "        \n",
    "        while not done:\n",
    "            state = np.squeeze(state) # added for pendulum\n",
    "            action = get_action(state,model)\n",
    "            observation,reward,done,_ = env.step(action)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                states = np.array(states)\n",
    "                actions = np.array(actions)\n",
    "                rewards = np.array(rewards)\n",
    "                if episode % 100 == 0:\n",
    "                    print(\"Episode: \",episode, \"Reward: \",rewards.sum())\n",
    "                train(states,actions,rewards,model_train)\n",
    "                \n",
    "        \n",
    "            state = observation\n",
    "            \n",
    "        \n",
    "        \n",
    "def test(env,model):\n",
    "    for episode in range(1,test_episodes+1):\n",
    "        done = False\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action = get_action(state,model,1)\n",
    "            observation,reward,done,_ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                rewards = np.array(rewards)\n",
    "                print(\"Episode: \",episode, \"Reward: \",rewards.sum())\n",
    "            state = observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 2000\n",
    "gamma = 0.99\n",
    "test_episodes = 10\n",
    "total_actions = cartpole_env.action_space.n\n",
    "input_dim = cartpole_env.observation_space.shape\n",
    "output_dim = total_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Episode:  100 Reward:  19.0\n",
      "Episode:  200 Reward:  14.0\n",
      "Episode:  300 Reward:  44.0\n",
      "Episode:  400 Reward:  50.0\n",
      "Episode:  500 Reward:  33.0\n",
      "Episode:  600 Reward:  22.0\n",
      "Episode:  700 Reward:  25.0\n",
      "Episode:  800 Reward:  30.0\n",
      "Episode:  900 Reward:  18.0\n",
      "Episode:  1000 Reward:  30.0\n",
      "Episode:  1100 Reward:  88.0\n",
      "Episode:  1200 Reward:  153.0\n",
      "Episode:  1300 Reward:  73.0\n",
      "Episode:  1400 Reward:  200.0\n",
      "Episode:  1500 Reward:  154.0\n",
      "Episode:  1600 Reward:  48.0\n",
      "Episode:  1700 Reward:  166.0\n",
      "Episode:  1800 Reward:  200.0\n",
      "Episode:  1900 Reward:  200.0\n",
      "Episode:  2000 Reward:  200.0\n"
     ]
    }
   ],
   "source": [
    "model = create_model(input_dim,output_dim)\n",
    "model_train = build_train_fn(model)  #customized fit function\n",
    "print(\"Training\")\n",
    "reinforce(cartpole_env,model,model_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n",
      "Episode:  1 Reward:  200.0\n",
      "Episode:  2 Reward:  200.0\n",
      "Episode:  3 Reward:  200.0\n",
      "Episode:  4 Reward:  200.0\n",
      "Episode:  5 Reward:  200.0\n",
      "Episode:  6 Reward:  200.0\n",
      "Episode:  7 Reward:  200.0\n",
      "Episode:  8 Reward:  200.0\n",
      "Episode:  9 Reward:  200.0\n",
      "Episode:  10 Reward:  200.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing\")\n",
    "test(cartpole_env,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pendulum_env = gym.make('Pendulum-v0')\n",
    "episodes = 2000\n",
    "gamma = 0.99\n",
    "test_episodes = 10\n",
    "total_actions = 1\n",
    "input_dim = pendulum_env.observation_space.shape\n",
    "output_dim = total_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Episode:  100 Reward:  -1366.2188\n",
      "Episode:  200 Reward:  -1251.4907\n",
      "Episode:  300 Reward:  -1371.6742\n",
      "Episode:  400 Reward:  -1214.8118\n",
      "Episode:  500 Reward:  -1256.9417\n",
      "Episode:  600 Reward:  -1324.7427\n",
      "Episode:  700 Reward:  -1425.9497\n",
      "Episode:  800 Reward:  -1372.248\n",
      "Episode:  900 Reward:  -1225.1758\n",
      "Episode:  1000 Reward:  -1797.7625\n",
      "Episode:  1100 Reward:  -1827.6471\n",
      "Episode:  1200 Reward:  -1253.9158\n",
      "Episode:  1300 Reward:  -1679.7559\n",
      "Episode:  1400 Reward:  -1345.5474\n",
      "Episode:  1500 Reward:  -1168.6304\n",
      "Episode:  1600 Reward:  -1326.5498\n",
      "Episode:  1700 Reward:  -1270.9841\n",
      "Episode:  1800 Reward:  -1555.3312\n",
      "Episode:  1900 Reward:  -1770.3217\n",
      "Episode:  2000 Reward:  -978.4465\n"
     ]
    }
   ],
   "source": [
    "pend_model = create_model(input_dim,output_dim)\n",
    "pend_model_train = build_train_fn(pend_model)  #customized fit function\n",
    "print(\"Training\")\n",
    "reinforce(pendulum_env,pend_model,pend_model_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import random\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteEnvironment(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def step(self,action):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        pass\n",
    "    \n",
    "class DiscreteAgent(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self,env):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def update(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_action(self,state):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(DiscreteEnvironment):\n",
    "    def __init__(self,r,c):\n",
    "        self.maxrow = r\n",
    "        self.maxcol = c\n",
    "        self.tot_act = 4\n",
    "        self.actions = [0,1,2,3]\n",
    "        self.reset()  \n",
    "        \n",
    "    def foul_state(self,row,col):\n",
    "        if row < 0 or row >= self.maxrow or col < 0 or col >= self.maxcol:\n",
    "            return 1\n",
    "        return 0\n",
    "    \n",
    "    def step(self,action):\n",
    "        row_copy = self.navigate_row\n",
    "        col_copy = self.navigate_col\n",
    "        \n",
    "        if self.navigate_row == self.goal_row and self.navigate_col == self.goal_col:\n",
    "            reward = 10\n",
    "            done = True\n",
    "            state = (self.navigate_row,self.navigate_col)\n",
    "        else:\n",
    "            if action == 0:\n",
    "                self.navigate_row -= 1\n",
    "            elif action == 1:\n",
    "                self.navigate_row += 1\n",
    "            elif action == 2:\n",
    "                self.navigate_col -= 1\n",
    "            elif action == 3:\n",
    "                self.navigate_col += 1\n",
    "\n",
    "            if self.foul_state(self.navigate_row,self.navigate_col): \n",
    "                reward = -10\n",
    "                done = True\n",
    "                state = (row_copy,col_copy)\n",
    "                #print(state)\n",
    "            elif self.navigate_row == self.goal_row and self.navigate_col == self.goal_col:\n",
    "                reward = 10\n",
    "                done = True\n",
    "                state = (self.navigate_row,self.navigate_col)\n",
    "            else: \n",
    "                reward = 0\n",
    "                done = False\n",
    "                state = (self.navigate_row,self.navigate_col)\n",
    "        \n",
    "        return state,reward,done\n",
    "    \n",
    "    '''\n",
    "    Used for dynamic programming methods where the start state is selected iteratively\n",
    "    '''\n",
    "    def set_start_state(self,state):\n",
    "        self.navigate_row = state[0]\n",
    "        self.navigate_col = state[1]\n",
    "        \n",
    "    '''\n",
    "    Initialize random start and goal state\n",
    "    '''\n",
    "    def reset(self):\n",
    "        \n",
    "        self.start_row = 0#random.randint(0,self.maxrow-1)\n",
    "        self.start_col = 0#random.randint(0,self.maxcol-1)\n",
    "        self.goal_row = 7#random.randint(0,self.maxrow-1)\n",
    "        self.goal_col = 7#random.randint(0,self.maxcol-1)\n",
    "        self.navigate_row = self.start_row\n",
    "        self.navigate_col = self.start_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIteration(DiscreteAgent):\n",
    "    def __init__(self,env):\n",
    "        self.env = env\n",
    "        self.gamma = 0.9\n",
    "        self.Q = np.zeros((env.maxrow,env.maxcol,env.tot_act))\n",
    "        self.V = np.zeros((env.maxrow,env.maxcol))\n",
    "        \n",
    "    def update(self):\n",
    "        for row in range(len(self.V)):\n",
    "            for col in range(len(self.V[row])):\n",
    "                state=(row,col)\n",
    "                for action in env.actions:\n",
    "                    env.set_start_state(state)\n",
    "                    new_state,reward,done = env.step(action)\n",
    "                    #print(reward)\n",
    "                    self.Q[state][action] = reward + self.gamma * self.V[new_state]\n",
    "                self.V[state] = np.max(self.Q[state])\n",
    "    \n",
    "    def get_action(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration(DiscreteAgent):\n",
    "    def __init__(self,env):\n",
    "        self.env = env\n",
    "        self.gamma = 0.9\n",
    "        self.Q = np.zeros((env.maxrow,env.maxcol,env.tot_act))\n",
    "        self.V = np.zeros((env.maxrow,env.maxcol))\n",
    "        self.policy = np.random.randint(env.tot_act,size=(env.maxrow,env.maxcol))\n",
    "        self.policy_stable = False\n",
    "    \n",
    "    def update(self):\n",
    "        while not self.policy_stable:\n",
    "            self.policy_evaluation()\n",
    "            self.policy_improvement()\n",
    "    \n",
    "    def policy_evaluation(self):\n",
    "        eps = 1e-10\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for row in range(len(self.V)):\n",
    "                for col in range(len(self.V[row])):\n",
    "                    state=(row,col)\n",
    "                    v = self.V[state]\n",
    "                    env.set_start_state(state)\n",
    "                    action = self.policy[state]\n",
    "                    new_state,reward,done = env.step(action)\n",
    "                    self.Q[state][action] = reward + self.gamma * self.V[new_state]\n",
    "                    self.V[state] = self.Q[state][action]\n",
    "                    delta = max(delta,np.abs(v-self.V[state]))\n",
    "                    \n",
    "            if delta < eps:\n",
    "                break\n",
    "            \n",
    "    def policy_improvement(self):\n",
    "        self.policy_stable = True\n",
    "        for row in range(len(self.V)):\n",
    "            for col in range(len(self.V[row])):\n",
    "                state=(row,col)\n",
    "                v = self.V[state]\n",
    "                for action in env.actions:\n",
    "                    env.set_start_state(state)\n",
    "                    new_state,reward,done = env.step(action)\n",
    "                    self.Q[state][action] = reward + self.gamma * self.V[new_state]\n",
    "                #if v != np.max(self.Q[state]):\n",
    "                self.V[state] = np.max(self.Q[state])\n",
    "                if np.argmax(self.Q[state]) != self.policy[state]:\n",
    "                    self.policy[state] = np.argmax(self.Q[state])\n",
    "                    self.policy_stable = False\n",
    "                    \n",
    "    \n",
    "    def get_action(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start =  0   0\n",
      "goal =  7   7\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld(8,8)\n",
    "print('start = ',env.start_row,' ',env.start_col)\n",
    "print('goal = ',env.goal_row,' ',env.goal_col)\n",
    "agent  = ValueIteration(env)\n",
    "\n",
    "\n",
    "sweep_no,max_sweeps = 0,10000\n",
    "while sweep_no < max_sweeps:\n",
    "    #print('new_update')\n",
    "    agent.update()\n",
    "    sweep_no+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 32.87679245,  35.41865828,  38.24295365,  41.38105961,\n",
       "         44.86784401,  48.7420489 ,  53.046721  ,  57.82969   ],\n",
       "       [ 35.41865828,  38.24295365,  41.38105961,  44.86784401,\n",
       "         48.7420489 ,  53.046721  ,  57.82969   ,  63.1441    ],\n",
       "       [ 38.24295365,  41.38105961,  44.86784401,  48.7420489 ,\n",
       "         53.046721  ,  57.82969   ,  63.1441    ,  69.049     ],\n",
       "       [ 41.38105961,  44.86784401,  48.7420489 ,  53.046721  ,\n",
       "         57.82969   ,  63.1441    ,  69.049     ,  75.61      ],\n",
       "       [ 44.86784401,  48.7420489 ,  53.046721  ,  57.82969   ,\n",
       "         63.1441    ,  69.049     ,  75.61      ,  82.9       ],\n",
       "       [ 48.7420489 ,  53.046721  ,  57.82969   ,  63.1441    ,\n",
       "         69.049     ,  75.61      ,  82.9       ,  91.        ],\n",
       "       [ 53.046721  ,  57.82969   ,  63.1441    ,  69.049     ,\n",
       "         75.61      ,  82.9       ,  91.        , 100.        ],\n",
       "       [ 57.82969   ,  63.1441    ,  69.049     ,  75.61      ,\n",
       "         82.9       ,  91.        , 100.        , 100.        ]])"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poilcy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start =  0   0\n",
      "goal =  7   7\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld(8,8)\n",
    "print('start = ',env.start_row,' ',env.start_col)\n",
    "print('goal = ',env.goal_row,' ',env.goal_col)\n",
    "agent  = PolicyIteration(env)\n",
    "#print(agent.V)\n",
    "#print(agent.Q)\n",
    "#print(agent.policy)\n",
    "agent.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [3 3 3 3 3 3 3 0]]\n"
     ]
    }
   ],
   "source": [
    "print(agent.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
